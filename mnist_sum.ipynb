{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import `pyswip` and consult the Prolog background knowledge base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyswip import Prolog\n",
    "prolog = Prolog()\n",
    "prolog.consult('mnist_sum.pl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test if the `CLP(FD)`-based abduction in the background knowledge base works correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 adds 9 equals 9.\n",
      "1 adds 8 equals 9.\n",
      "2 adds 7 equals 9.\n",
      "3 adds 6 equals 9.\n",
      "4 adds 5 equals 9.\n",
      "5 adds 4 equals 9.\n",
      "6 adds 3 equals 9.\n",
      "7 adds 2 equals 9.\n",
      "8 adds 1 equals 9.\n",
      "9 adds 0 equals 9.\n"
     ]
    }
   ],
   "source": [
    "target = 9\n",
    "for soln in prolog.query(\"abduce([X,Y], {})\".format(target)):\n",
    "    print(soln[\"X\"], \"adds\", soln[\"Y\"], \"equals {}.\".format(target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abductive Learning\n",
    "\n",
    "Now let's try to implement the MNIST sum learning algorithm using the Abductive Learning framework.\n",
    "\n",
    "### Dataset Generation\n",
    "\n",
    "Directly copy the codes from the `data_generator.ipynb` notebook file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "transform=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "dataset1 = datasets.MNIST('data', train=True, download=True,\n",
    "                            transform=transform)\n",
    "dataset2 = datasets.MNIST('data', train=False,\n",
    "                            transform=transform)\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "digit_groups_train = {0:[], 1:[], 2:[], 3:[], 4:[], 5:[], 6:[], 7:[], 8:[], 9:[]}\n",
    "digit_groups_test = {0:[], 1:[], 2:[], 3:[], 4:[], 5:[], 6:[], 7:[], 8:[], 9:[]}\n",
    "\n",
    "for i in range(len(dataset1)): \n",
    "    digit_groups_train[int(dataset1.targets[i])].append(i)\n",
    "for i in range(len(dataset2)): \n",
    "    digit_groups_test[int(dataset2.targets[i])].append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_Sum:\n",
    "    def __init__(self, num, digit_groups):\n",
    "        self.targets = []\n",
    "        self.img_indices = []\n",
    "        self.ground_truth = []\n",
    "        self.length = num\n",
    "        for i in range(num):\n",
    "            # sampling two numbers from 0 to 9\n",
    "            sampled_digits = np.random.choice(10, 2)\n",
    "            self.ground_truth.append(list(sampled_digits))\n",
    "\n",
    "            # using the sum of the sampled digits as the target\n",
    "            self.targets.append(sum(sampled_digits))\n",
    "            ids = []\n",
    "            for j in range(len(sampled_digits)):\n",
    "                # get the j-th digits\n",
    "                digit = sampled_digits[j]\n",
    "                # total number of the images of the digit\n",
    "                ids.append(np.random.choice(digit_groups[digit]))\n",
    "            self.img_indices.append(ids)\n",
    "\n",
    "# Generate the training and test dataset for MNIST Sum task\n",
    "mnist_sum_data_train = MNIST_Sum(3000, digit_groups_train)\n",
    "mnist_sum_data_test = MNIST_Sum(3000, digit_groups_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Machine Learning Part\n",
    "\n",
    "Neural networks for image classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)\n",
    "\n",
    "\n",
    "def conv_net(outdim, *args, **kwargs):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(1, 32, 3, 1),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(32, 64, 3, 1),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(2),\n",
    "        nn.Dropout(0.25),\n",
    "        Flatten(),\n",
    "        nn.Linear(9216, 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.5),\n",
    "        nn.Linear(128, outdim),\n",
    "        nn.LogSoftmax(dim=1)\n",
    "    )\n",
    "\n",
    "\n",
    "def auto_enc(outdim, *args, **kwargs):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(outdim, 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128, 784)\n",
    "    )\n",
    "\n",
    "\n",
    "def mlp(indim, outdim, *args, **kwargs):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(indim, 32),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(32, outdim),\n",
    "        nn.LogSoftmax(dim=1),\n",
    "    )\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    \"\"\"A (Bi)LSTM Model.\n",
    "\n",
    "    Attributes:\n",
    "        num_layers: the number of LSTM layers (number of stacked LSTM models) in the network.\n",
    "        in_dim: the size of the input sample.\n",
    "        hidden_dim: the size of the hidden layers.\n",
    "        out_dim: the size of the output.\n",
    "        activation: the activation function.\n",
    "        bidirectional: the flag for bidirectional LSTM\n",
    "        dropout: the dropout rate if num_layers > 1\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_layers, in_dim, hidden_dim, out_dim,\n",
    "                 bidirectional=False, dropout=0):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.in_dim = in_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.bidirectional = bidirectional\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.lstm = nn.LSTM(self.in_dim,\n",
    "                            self.hidden_dim,\n",
    "                            num_layers=self.num_layers,\n",
    "                            bidirectional=self.bidirectional,\n",
    "                            dropout=self.dropout,\n",
    "                            batch_first=True)\n",
    "        fc_dim = self.hidden_dim * 2 if self.bidirectional else self.hidden_dim\n",
    "        self.fc = nn.Linear(fc_dim, self.out_dim)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        lstm_out, _ = self.lstm(inputs)\n",
    "        outputs = self.fc(lstm_out[:, -1, :])\n",
    "        outputs = torch.sigmoid(outputs)\n",
    "        return outputs\n",
    "\n",
    "    def loss_function(self, pred, y):\n",
    "        return F.binary_cross_entropy(pred, y.view(y.shape[0], -1))\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    outdim = 10\n",
    "\n",
    "    def __init__(self, outdim):\n",
    "        super(Net, self).__init__()\n",
    "        self.outdim = outdim\n",
    "        self.enc = conv_net(outdim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.enc(x)\n",
    "        return output\n",
    "\n",
    "    def loss_function(self, pred, y):\n",
    "        return F.nll_loss(pred, y)\n",
    "\n",
    "def train(model, device, train_loader, optimizer, epoch,\n",
    "          log_interval=1000, dry_run=False):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = model.loss_function(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "            if dry_run:\n",
    "                break\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            # sum up batch loss\n",
    "            test_loss += model.loss_function(output, target).item()\n",
    "            # get the index of the max log-probability\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('-- Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Logic Abduction Part\n",
    "\n",
    "It involves the following steps:\n",
    "1. Given the target (label, i.e., the sum of the two images), using `pyswip` to abduce possible pseudo-labels for them.\n",
    "2. Calculate the probability of each pair of pseudo-labels.\n",
    "3. Return the most probable pseudo-labels to retrain the neural network.\n",
    "\n",
    "_Remark_: For more complicated problems, a better way of searching for the best pseudo-labels is required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Abducing possible pseudo-labels given the sum\n",
    "\n",
    "- `pl` is the Prolog instance that consulted `mnist_sum.pl`;\n",
    "- `target` is the sum of two images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def abduce(pl, target):\n",
    "    # This abduce/2 function is defined in \"mnist_sum.pl\"\n",
    "    ans = [];\n",
    "    for soln in pl.query(\"abduce([X,Y], {})\".format(target)):\n",
    "        ans.append([soln[\"X\"], soln[\"Y\"]])\n",
    "    if len(ans) > 0:\n",
    "        return ans\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the `abduce` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6, 9], [7, 8], [8, 7], [9, 6]]\n"
     ]
    }
   ],
   "source": [
    "print(abduce(prolog, 15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The Abductive Learning Procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing useful libraries and set the default neural network training parameters within the Abductive Learning Process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "nn_train_kwargs = {'batch_size': 64, 'shuffle': True}\n",
    "nn_epoch = 2\n",
    "\n",
    "nn_test_loader = torch.utils.data.DataLoader(dataset2, **nn_train_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful functions for abduction:\n",
    "1. `get_mnist_imgs`: given `indices`, sample a subset of images from `dataset` (such as the `MNIST` dataset).\n",
    "2. `best_pseudo_label`: given a set of abduced possible pseudo-labels and the pseudo-label distribution, return the most probable pseudo-label combination for each image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mnist_imgs(dataset, indices, use_cuda=False):\n",
    "    \"\"\"\n",
    "    Given get the image tensor from mnist dataset by indices\n",
    "    \"\"\"\n",
    "\n",
    "    n = len(indices)\n",
    "    img_tensor, tgt = dataset[indices[0]]\n",
    "    img_tensor = torch.reshape(img_tensor, (1, 1, 28, 28))\n",
    "    targets = [tgt]\n",
    "    i = 1\n",
    "    while i < n:\n",
    "        img, tgt = dataset[indices[i]]\n",
    "        img = torch.reshape(img, (1, 1, 28, 28))\n",
    "        img_tensor = torch.cat((img_tensor, img), 0)\n",
    "        targets.append(tgt)\n",
    "        i = i + 1\n",
    "    if use_cuda:\n",
    "        img_tensor = img_tensor.to(torch.device(\"cuda\"))\n",
    "    return img_tensor, targets\n",
    "\n",
    "def best_pseudo_label(pseudo_label_lists, pseudo_label_dist):\n",
    "    best_score = -100000.0\n",
    "    best_combi = np.zeros(pseudo_label_dist.shape[0])\n",
    "    probabilities = np.exp(pseudo_label_dist)\n",
    "    for label_combi in pseudo_label_lists:\n",
    "        # because the scores are log_softmax, the log probability can be calculated as sum\n",
    "        score = 1.0\n",
    "        for j in range(len(label_combi)):\n",
    "            score = score*probabilities[j, label_combi[j]]\n",
    "        if score >= best_score:\n",
    "            best_score = score\n",
    "            best_combi = label_combi\n",
    "    return best_combi, score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main procedure for abductive Learning. Given a machine learning `model` and a prolog instance `pl` with `dataset`, it does the following steps:\n",
    "1. Using `model` to predict the pseudo-label probabilistic distribution of `dataset`;\n",
    "2. Finding the best pseudo-label combination considering both the abduction result from `pl` and the pseudo-label probabilistic distribution;\n",
    "3. Retrain the neural network with the abduced pseudo-labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ABL_main(model, pl, dataset, optimizer=None, scheduler=None):\n",
    "    # number of examples\n",
    "    num_examples = dataset.length\n",
    "    abduced_data_ids = []\n",
    "    abduced_labels = []\n",
    "    ground_truth_labels = []\n",
    "\n",
    "    # start abduction\n",
    "    for i in tqdm (range(num_examples), desc=\"Abducing...\"):\n",
    "        target = int(dataset.targets[i])\n",
    "        possible_pseudo_labels = abduce(pl, target)\n",
    "        if possible_pseudo_labels is not None:\n",
    "            # reshape the tensor of the two MNIST images to match NN model's input dimensions\n",
    "            img_indices = dataset.img_indices[i]\n",
    "            imgs, _ = get_mnist_imgs(dataset1, img_indices, use_cuda=False)\n",
    "\n",
    "            pseudo_label_distribution = model(imgs).detach().numpy()\n",
    "\n",
    "            # find the pseudo-labels with the maximum likelihood\n",
    "            abduced_pseudo_labels, _ = best_pseudo_label(possible_pseudo_labels, pseudo_label_distribution)\n",
    "\n",
    "            # for abduced dataset\n",
    "            abduced_data_ids = abduced_data_ids + img_indices\n",
    "            abduced_labels = abduced_labels + abduced_pseudo_labels\n",
    "            ground_truth_labels = ground_truth_labels + dataset.ground_truth[i]\n",
    "\n",
    "    # changing the training data labels to the abduced labels\n",
    "    for i, img in enumerate(abduced_data_ids):\n",
    "        dataset1.targets[img] = abduced_labels[i]\n",
    "    \n",
    "    abduction_accuracy = np.sum(np.array(ground_truth_labels) == np.array(abduced_labels))/len(abduced_labels)\n",
    "\n",
    "\n",
    "    # making new dataset with abduced labels\n",
    "    abduced_data = torch.utils.data.Subset(dataset1, abduced_data_ids)\n",
    "\n",
    "    # training the neural network model\n",
    "    abduced_train_loader = torch.utils.data.DataLoader(abduced_data, batch_size=64)\n",
    "\n",
    "    for epoch in range(1, nn_epoch + 1):\n",
    "        train(model, device, abduced_train_loader, optimizer, epoch)\n",
    "        print(\"Abduction accuracy: \", abduction_accuracy)\n",
    "        scheduler.step()\n",
    "    test(model, device, nn_test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialise model and optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net(outdim=10).to(device)\n",
    "\n",
    "optimizer = optim.Adadelta(model.parameters(), lr=1.0)\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run Abductive Learning without any pre-train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Abducing...: 100%|██████████| 3000/3000 [00:03<00:00, 784.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6000 (0%)]\tLoss: 2.239590\n",
      "Abduction accuracy:  0.19266666666666668\n",
      "Train Epoch: 2 [0/6000 (0%)]\tLoss: 1.980804\n",
      "Abduction accuracy:  0.19266666666666668\n",
      "-- Test set: Average loss: 0.0344, Accuracy: 3212/10000 (32%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Abducing...: 100%|██████████| 3000/3000 [00:03<00:00, 887.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6000 (0%)]\tLoss: 1.724332\n",
      "Abduction accuracy:  0.391\n",
      "Train Epoch: 2 [0/6000 (0%)]\tLoss: 1.309425\n",
      "Abduction accuracy:  0.391\n",
      "-- Test set: Average loss: 0.0325, Accuracy: 5904/10000 (59%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Abducing...: 100%|██████████| 3000/3000 [00:03<00:00, 880.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6000 (0%)]\tLoss: 1.089493\n",
      "Abduction accuracy:  0.5246666666666666\n",
      "Train Epoch: 2 [0/6000 (0%)]\tLoss: 0.876822\n",
      "Abduction accuracy:  0.5246666666666666\n",
      "-- Test set: Average loss: 0.0296, Accuracy: 6792/10000 (68%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Abducing...: 100%|██████████| 3000/3000 [00:03<00:00, 878.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6000 (0%)]\tLoss: 0.820825\n",
      "Abduction accuracy:  0.5773333333333334\n",
      "Train Epoch: 2 [0/6000 (0%)]\tLoss: 0.674790\n",
      "Abduction accuracy:  0.5773333333333334\n",
      "-- Test set: Average loss: 0.0273, Accuracy: 7123/10000 (71%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Abducing...: 100%|██████████| 3000/3000 [00:03<00:00, 886.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6000 (0%)]\tLoss: 0.686643\n",
      "Abduction accuracy:  0.6076666666666667\n",
      "Train Epoch: 2 [0/6000 (0%)]\tLoss: 0.686003\n",
      "Abduction accuracy:  0.6076666666666667\n",
      "-- Test set: Average loss: 0.0256, Accuracy: 7245/10000 (72%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Abducing...: 100%|██████████| 3000/3000 [00:03<00:00, 875.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6000 (0%)]\tLoss: 0.659674\n",
      "Abduction accuracy:  0.64\n",
      "Train Epoch: 2 [0/6000 (0%)]\tLoss: 0.741770\n",
      "Abduction accuracy:  0.64\n",
      "-- Test set: Average loss: 0.0239, Accuracy: 7332/10000 (73%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Abducing...: 100%|██████████| 3000/3000 [00:03<00:00, 886.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6000 (0%)]\tLoss: 0.641245\n",
      "Abduction accuracy:  0.6753333333333333\n",
      "Train Epoch: 2 [0/6000 (0%)]\tLoss: 0.647668\n",
      "Abduction accuracy:  0.6753333333333333\n",
      "-- Test set: Average loss: 0.0224, Accuracy: 7459/10000 (75%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Abducing...: 100%|██████████| 3000/3000 [00:03<00:00, 879.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6000 (0%)]\tLoss: 0.655451\n",
      "Abduction accuracy:  0.7093333333333334\n",
      "Train Epoch: 2 [0/6000 (0%)]\tLoss: 0.593131\n",
      "Abduction accuracy:  0.7093333333333334\n",
      "-- Test set: Average loss: 0.0214, Accuracy: 7652/10000 (77%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Abducing...: 100%|██████████| 3000/3000 [00:03<00:00, 883.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6000 (0%)]\tLoss: 0.632935\n",
      "Abduction accuracy:  0.731\n",
      "Train Epoch: 2 [0/6000 (0%)]\tLoss: 0.681322\n",
      "Abduction accuracy:  0.731\n",
      "-- Test set: Average loss: 0.0209, Accuracy: 7764/10000 (78%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Abducing...: 100%|██████████| 3000/3000 [00:03<00:00, 798.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6000 (0%)]\tLoss: 0.635141\n",
      "Abduction accuracy:  0.74\n",
      "Train Epoch: 2 [0/6000 (0%)]\tLoss: 0.560081\n",
      "Abduction accuracy:  0.74\n",
      "-- Test set: Average loss: 0.0207, Accuracy: 7823/10000 (78%)\n"
     ]
    }
   ],
   "source": [
    "ABL_epochs = 10\n",
    "for epoch in range(ABL_epochs):\n",
    "    ABL_main(model, prolog, mnist_sum_data_train, optimizer=optimizer, scheduler=scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run Abductive Learning with one-shot pre-train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample an one-shot training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/10 (0%)]\tLoss: 2.325954\n",
      "Train Epoch: 2 [0/10 (0%)]\tLoss: 2.169095\n",
      "Train Epoch: 3 [0/10 (0%)]\tLoss: 1.849946\n",
      "Train Epoch: 4 [0/10 (0%)]\tLoss: 1.627764\n",
      "-- Test set: Average loss: 0.0299, Accuracy: 5191/10000 (52%)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# reset the machine learning model\n",
    "model = Net(outdim=10).to(device)\n",
    "\n",
    "# reset dataset1 to reset the labels for one-shot training, \n",
    "# since the previous abductive learning process has changed \n",
    "# the ground truth labels in dataset1\n",
    "dataset1 = datasets.MNIST('data', train=True, download=True,\n",
    "                            transform=transform)\n",
    "\n",
    "n_samples = 1\n",
    "few_shot_indices = []\n",
    "\n",
    "for i in range(10):\n",
    "    few_shot_indices = few_shot_indices + \\\n",
    "        random.sample(digit_groups_train[i], n_samples)\n",
    "\n",
    "# few_shot_indices = random.sample(all_img_indices, n_samples)\n",
    "\n",
    "sup_imgs_train = torch.utils.data.Subset(dataset1, few_shot_indices)\n",
    "\n",
    "sup_train_loader = torch.utils.data.DataLoader(\n",
    "    sup_imgs_train, **nn_train_kwargs)\n",
    "\n",
    "optimizer = optim.Adadelta(model.parameters(), lr=1.0)\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=0.7)\n",
    "\n",
    "for epoch in range(1, 5):\n",
    "    train(model, device, sup_train_loader,\n",
    "        optimizer, epoch)\n",
    "    #test(model, device, nn_test_loader)\n",
    "    scheduler.step()\n",
    "test(model, device, nn_test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Abducing...: 100%|██████████| 3000/3000 [00:03<00:00, 872.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6000 (0%)]\tLoss: 2.063701\n",
      "Abduction accuracy:  0.6893333333333334\n",
      "Train Epoch: 2 [0/6000 (0%)]\tLoss: 1.181165\n",
      "Abduction accuracy:  0.6893333333333334\n",
      "-- Test set: Average loss: 0.0104, Accuracy: 8054/10000 (81%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Abducing...: 100%|██████████| 3000/3000 [00:03<00:00, 841.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6000 (0%)]\tLoss: 0.582318\n",
      "Abduction accuracy:  0.9386666666666666\n",
      "Train Epoch: 2 [0/6000 (0%)]\tLoss: 0.399606\n",
      "Abduction accuracy:  0.9386666666666666\n",
      "-- Test set: Average loss: 0.0038, Accuracy: 9355/10000 (94%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Abducing...: 100%|██████████| 3000/3000 [00:03<00:00, 881.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6000 (0%)]\tLoss: 0.236707\n",
      "Abduction accuracy:  0.99\n",
      "Train Epoch: 2 [0/6000 (0%)]\tLoss: 0.176531\n",
      "Abduction accuracy:  0.99\n",
      "-- Test set: Average loss: 0.0027, Accuracy: 9500/10000 (95%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Abducing...: 100%|██████████| 3000/3000 [00:03<00:00, 830.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6000 (0%)]\tLoss: 0.154634\n",
      "Abduction accuracy:  0.9933333333333333\n",
      "Train Epoch: 2 [0/6000 (0%)]\tLoss: 0.155512\n",
      "Abduction accuracy:  0.9933333333333333\n",
      "-- Test set: Average loss: 0.0024, Accuracy: 9550/10000 (96%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Abducing...: 100%|██████████| 3000/3000 [00:03<00:00, 769.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6000 (0%)]\tLoss: 0.110727\n",
      "Abduction accuracy:  0.9943333333333333\n",
      "Train Epoch: 2 [0/6000 (0%)]\tLoss: 0.130157\n",
      "Abduction accuracy:  0.9943333333333333\n",
      "-- Test set: Average loss: 0.0023, Accuracy: 9561/10000 (96%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Abducing...: 100%|██████████| 3000/3000 [00:03<00:00, 781.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6000 (0%)]\tLoss: 0.096415\n",
      "Abduction accuracy:  0.9946666666666667\n",
      "Train Epoch: 2 [0/6000 (0%)]\tLoss: 0.094798\n",
      "Abduction accuracy:  0.9946666666666667\n",
      "-- Test set: Average loss: 0.0023, Accuracy: 9573/10000 (96%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Abducing...: 100%|██████████| 3000/3000 [00:03<00:00, 830.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6000 (0%)]\tLoss: 0.139709\n",
      "Abduction accuracy:  0.995\n",
      "Train Epoch: 2 [0/6000 (0%)]\tLoss: 0.074736\n",
      "Abduction accuracy:  0.995\n",
      "-- Test set: Average loss: 0.0023, Accuracy: 9574/10000 (96%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Abducing...: 100%|██████████| 3000/3000 [00:03<00:00, 861.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6000 (0%)]\tLoss: 0.128482\n",
      "Abduction accuracy:  0.995\n",
      "Train Epoch: 2 [0/6000 (0%)]\tLoss: 0.130535\n",
      "Abduction accuracy:  0.995\n",
      "-- Test set: Average loss: 0.0023, Accuracy: 9577/10000 (96%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Abducing...: 100%|██████████| 3000/3000 [00:03<00:00, 863.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6000 (0%)]\tLoss: 0.056206\n",
      "Abduction accuracy:  0.995\n",
      "Train Epoch: 2 [0/6000 (0%)]\tLoss: 0.065177\n",
      "Abduction accuracy:  0.995\n",
      "-- Test set: Average loss: 0.0023, Accuracy: 9578/10000 (96%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Abducing...: 100%|██████████| 3000/3000 [00:03<00:00, 750.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6000 (0%)]\tLoss: 0.077335\n",
      "Abduction accuracy:  0.995\n",
      "Train Epoch: 2 [0/6000 (0%)]\tLoss: 0.110283\n",
      "Abduction accuracy:  0.995\n",
      "-- Test set: Average loss: 0.0023, Accuracy: 9578/10000 (96%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Abducing...: 100%|██████████| 3000/3000 [00:03<00:00, 860.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6000 (0%)]\tLoss: 0.142133\n",
      "Abduction accuracy:  0.995\n",
      "Train Epoch: 2 [0/6000 (0%)]\tLoss: 0.091053\n",
      "Abduction accuracy:  0.995\n",
      "-- Test set: Average loss: 0.0023, Accuracy: 9578/10000 (96%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Abducing...: 100%|██████████| 3000/3000 [00:03<00:00, 872.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6000 (0%)]\tLoss: 0.056301\n",
      "Abduction accuracy:  0.995\n",
      "Train Epoch: 2 [0/6000 (0%)]\tLoss: 0.088079\n",
      "Abduction accuracy:  0.995\n",
      "-- Test set: Average loss: 0.0023, Accuracy: 9579/10000 (96%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Abducing...: 100%|██████████| 3000/3000 [00:03<00:00, 879.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6000 (0%)]\tLoss: 0.149324\n",
      "Abduction accuracy:  0.995\n",
      "Train Epoch: 2 [0/6000 (0%)]\tLoss: 0.037813\n",
      "Abduction accuracy:  0.995\n",
      "-- Test set: Average loss: 0.0023, Accuracy: 9578/10000 (96%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Abducing...: 100%|██████████| 3000/3000 [00:03<00:00, 858.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6000 (0%)]\tLoss: 0.079000\n",
      "Abduction accuracy:  0.995\n",
      "Train Epoch: 2 [0/6000 (0%)]\tLoss: 0.104606\n",
      "Abduction accuracy:  0.995\n",
      "-- Test set: Average loss: 0.0023, Accuracy: 9578/10000 (96%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Abducing...: 100%|██████████| 3000/3000 [00:03<00:00, 893.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6000 (0%)]\tLoss: 0.059887\n",
      "Abduction accuracy:  0.995\n",
      "Train Epoch: 2 [0/6000 (0%)]\tLoss: 0.154992\n",
      "Abduction accuracy:  0.995\n",
      "-- Test set: Average loss: 0.0023, Accuracy: 9578/10000 (96%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Abducing...: 100%|██████████| 3000/3000 [00:03<00:00, 872.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6000 (0%)]\tLoss: 0.120666\n",
      "Abduction accuracy:  0.995\n",
      "Train Epoch: 2 [0/6000 (0%)]\tLoss: 0.102399\n",
      "Abduction accuracy:  0.995\n",
      "-- Test set: Average loss: 0.0023, Accuracy: 9578/10000 (96%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Abducing...: 100%|██████████| 3000/3000 [00:03<00:00, 873.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6000 (0%)]\tLoss: 0.067268\n",
      "Abduction accuracy:  0.995\n",
      "Train Epoch: 2 [0/6000 (0%)]\tLoss: 0.050261\n",
      "Abduction accuracy:  0.995\n",
      "-- Test set: Average loss: 0.0023, Accuracy: 9578/10000 (96%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Abducing...: 100%|██████████| 3000/3000 [00:03<00:00, 787.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6000 (0%)]\tLoss: 0.102566\n",
      "Abduction accuracy:  0.995\n",
      "Train Epoch: 2 [0/6000 (0%)]\tLoss: 0.065478\n",
      "Abduction accuracy:  0.995\n",
      "-- Test set: Average loss: 0.0023, Accuracy: 9578/10000 (96%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Abducing...: 100%|██████████| 3000/3000 [00:03<00:00, 878.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6000 (0%)]\tLoss: 0.075812\n",
      "Abduction accuracy:  0.995\n",
      "Train Epoch: 2 [0/6000 (0%)]\tLoss: 0.083705\n",
      "Abduction accuracy:  0.995\n",
      "-- Test set: Average loss: 0.0023, Accuracy: 9578/10000 (96%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Abducing...: 100%|██████████| 3000/3000 [00:03<00:00, 869.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6000 (0%)]\tLoss: 0.101748\n",
      "Abduction accuracy:  0.995\n",
      "Train Epoch: 2 [0/6000 (0%)]\tLoss: 0.079467\n",
      "Abduction accuracy:  0.995\n",
      "-- Test set: Average loss: 0.0023, Accuracy: 9578/10000 (96%)\n"
     ]
    }
   ],
   "source": [
    "ABL_epochs = 20\n",
    "for epoch in range(ABL_epochs):\n",
    "    ABL_main(model, prolog, mnist_sum_data_train, optimizer=optimizer, scheduler=scheduler)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
