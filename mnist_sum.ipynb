{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import `pyswip` and consult the Prolog background knowledge base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyswip import Prolog\n",
    "prolog = Prolog()\n",
    "prolog.consult('mnist_sum.pl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test if the `CLP(FD)`-based abduction in the background knowledge base works correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 adds 9 equals 10.\n",
      "2 adds 8 equals 10.\n",
      "3 adds 7 equals 10.\n",
      "4 adds 6 equals 10.\n",
      "5 adds 5 equals 10.\n",
      "6 adds 4 equals 10.\n",
      "7 adds 3 equals 10.\n",
      "8 adds 2 equals 10.\n",
      "9 adds 1 equals 10.\n"
     ]
    }
   ],
   "source": [
    "target = 10\n",
    "for soln in prolog.query(\"abduce([X,Y], {})\".format(target)):\n",
    "    print(soln[\"X\"], \"adds\", soln[\"Y\"], \"equals {}.\".format(target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abductive Learning\n",
    "\n",
    "Now let's try to implement the MNIST sum learning algorithm using the Abductive Learning framework.\n",
    "\n",
    "### Dataset Generation\n",
    "\n",
    "Directly copy the codes from the `data_generator.ipynb` notebook file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "transform=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "dataset1 = datasets.MNIST('data', train=True, download=True,\n",
    "                            transform=transform)\n",
    "dataset2 = datasets.MNIST('data', train=False,\n",
    "                            transform=transform)\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "digit_groups_train = {0:[], 1:[], 2:[], 3:[], 4:[], 5:[], 6:[], 7:[], 8:[], 9:[]}\n",
    "digit_groups_test = {0:[], 1:[], 2:[], 3:[], 4:[], 5:[], 6:[], 7:[], 8:[], 9:[]}\n",
    "\n",
    "for i in range(len(dataset1)): \n",
    "    digit_groups_train[int(dataset1.targets[i])].append(i)\n",
    "for i in range(len(dataset2)): \n",
    "    digit_groups_test[int(dataset2.targets[i])].append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_Sum:\n",
    "    def __init__(self, num, digit_groups):\n",
    "        self.targets = []\n",
    "        self.img_indices = [] # list of lists of 2 ids\n",
    "        self.ground_truth = []\n",
    "        self.length = num\n",
    "        for i in range(num):\n",
    "            # sampling two numbers from 0 to 9\n",
    "            sampled_digits = np.random.choice(10, 2)\n",
    "            self.ground_truth.append(list(sampled_digits))\n",
    "\n",
    "            # using the sum of the sampled digits as the target\n",
    "            self.targets.append(sum(sampled_digits))\n",
    "            ids = []\n",
    "            for j in range(len(sampled_digits)):\n",
    "                # get the j-th digits\n",
    "                digit = sampled_digits[j]\n",
    "                # total number of the images of the digit\n",
    "                ids.append(np.random.choice(digit_groups[digit]))\n",
    "            self.img_indices.append(ids)\n",
    "\n",
    "# Generate the training and test dataset for MNIST Sum task\n",
    "mnist_sum_data_train = MNIST_Sum(3000, digit_groups_train)\n",
    "mnist_sum_data_test = MNIST_Sum(3000, digit_groups_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Machine Learning Part\n",
    "\n",
    "Neural networks for image classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)\n",
    "\n",
    "\n",
    "def conv_net(outdim, *args, **kwargs):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(1, 32, 3, 1),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(32, 64, 3, 1),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(2),\n",
    "        nn.Dropout(0.25),\n",
    "        Flatten(),\n",
    "        nn.Linear(9216, 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.5),\n",
    "        nn.Linear(128, outdim),\n",
    "        nn.LogSoftmax(dim=1)\n",
    "    )\n",
    "\n",
    "\n",
    "def auto_enc(outdim, *args, **kwargs):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(outdim, 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128, 784)\n",
    "    )\n",
    "\n",
    "\n",
    "def mlp(indim, outdim, *args, **kwargs):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(indim, 32),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(32, outdim),\n",
    "        nn.LogSoftmax(dim=1),\n",
    "    )\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    \"\"\"A (Bi)LSTM Model.\n",
    "\n",
    "    Attributes:\n",
    "        num_layers: the number of LSTM layers (number of stacked LSTM models) in the network.\n",
    "        in_dim: the size of the input sample.\n",
    "        hidden_dim: the size of the hidden layers.\n",
    "        out_dim: the size of the output.\n",
    "        activation: the activation function.\n",
    "        bidirectional: the flag for bidirectional LSTM\n",
    "        dropout: the dropout rate if num_layers > 1\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_layers, in_dim, hidden_dim, out_dim,\n",
    "                 bidirectional=False, dropout=0):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.in_dim = in_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.bidirectional = bidirectional\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.lstm = nn.LSTM(self.in_dim,\n",
    "                            self.hidden_dim,\n",
    "                            num_layers=self.num_layers,\n",
    "                            bidirectional=self.bidirectional,\n",
    "                            dropout=self.dropout,\n",
    "                            batch_first=True)\n",
    "        fc_dim = self.hidden_dim * 2 if self.bidirectional else self.hidden_dim\n",
    "        self.fc = nn.Linear(fc_dim, self.out_dim)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        lstm_out, _ = self.lstm(inputs)\n",
    "        outputs = self.fc(lstm_out[:, -1, :])\n",
    "        outputs = torch.sigmoid(outputs)\n",
    "        return outputs\n",
    "\n",
    "    def loss_function(self, pred, y):\n",
    "        return F.binary_cross_entropy(pred, y.view(y.shape[0], -1))\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    outdim = 10\n",
    "\n",
    "    def __init__(self, outdim):\n",
    "        super(Net, self).__init__()\n",
    "        self.outdim = outdim\n",
    "        self.enc = conv_net(outdim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.enc(x)\n",
    "        return output\n",
    "\n",
    "    def loss_function(self, pred, y):\n",
    "        return F.nll_loss(pred, y)\n",
    "\n",
    "def train(model, device, train_loader, optimizer, epoch,\n",
    "          log_interval=1000, dry_run=False):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = model.loss_function(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "            if dry_run:\n",
    "                break\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            # sum up batch loss\n",
    "            test_loss += model.loss_function(output, target).item()\n",
    "            # get the index of the max log-probability\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('-- Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Logic Abduction Part\n",
    "\n",
    "It involves the following steps:\n",
    "1. Given the target (label, i.e., the sum of the two images), using `pyswip` to abduce possible pseudo-labels for them.\n",
    "2. Calculate the probability of each pair of pseudo-labels.\n",
    "3. Return the most probable pseudo-labels to retrain the neural network.\n",
    "\n",
    "_Remark_: For more complicated problems, a better way of searching for the best pseudo-labels is required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Abducing possible pseudo-labels given the sum\n",
    "\n",
    "- `pl` is the Prolog instance that consulted `mnist_sum.pl`;\n",
    "- `target` is the sum of two images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def abduce(pl, target):\n",
    "    # This abduce/2 function is defined in \"mnist_sum.pl\"\n",
    "    ans = [];\n",
    "    for soln in pl.query(\"abduce([X,Y], {})\".format(target)): # given targer, abduce X and Y\n",
    "        ans.append([soln[\"X\"], soln[\"Y\"]])\n",
    "    if len(ans) > 0:\n",
    "        return ans\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the `abduce` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(abduce(prolog, 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The Abductive Learning Procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing useful libraries and set the default neural network training parameters within the Abductive Learning Process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "nn_train_kwargs = {'batch_size': 64, 'shuffle': True}\n",
    "nn_epoch = 4\n",
    "\n",
    "nn_test_loader = torch.utils.data.DataLoader(dataset2, **nn_train_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful functions for abduction:\n",
    "1. `get_mnist_imgs`: given `indices`, sample a subset of images from `dataset` (such as the `MNIST` dataset).\n",
    "2. `best_pseudo_label`: given a set of abduced possible pseudo-labels and the pseudo-label distribution, return the most probable pseudo-label combination for each image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mnist_imgs(dataset, indices, use_cuda=False):\n",
    "    \"\"\"\n",
    "    Get the image tensor from mnist dataset by indices\n",
    "    \"\"\"\n",
    "\n",
    "    n = len(indices)\n",
    "    img_tensor, tgt = dataset[indices[0]]\n",
    "    img_tensor = torch.reshape(img_tensor, (1, 1, 28, 28))\n",
    "    targets = [tgt]\n",
    "    i = 1\n",
    "    while i < n:\n",
    "        img, tgt = dataset[indices[i]]\n",
    "        img = torch.reshape(img, (1, 1, 28, 28))\n",
    "        img_tensor = torch.cat((img_tensor, img), 0)\n",
    "        targets.append(tgt)\n",
    "        i = i + 1\n",
    "    if use_cuda:\n",
    "        img_tensor = img_tensor.to(torch.device(\"cuda\"))\n",
    "    return img_tensor, targets # list of image tensors and list of its targets\n",
    "\n",
    "def best_pseudo_label(pseudo_label_lists, pseudo_label_dist):\n",
    "    \"\"\"\n",
    "    Given plabel lists and plabel distribution list\n",
    "    Return the best combination with score\n",
    "    \"\"\"\n",
    "    best_score = -100000.0\n",
    "    best_combi = np.zeros(pseudo_label_dist.shape[0])\n",
    "    probabilities = np.exp(pseudo_label_dist)\n",
    "    for label_combi in pseudo_label_lists:\n",
    "        # because the scores are log_softmax, the log probability can be calculated as sum\n",
    "        score = 1.0\n",
    "        for j in range(len(label_combi)):\n",
    "            score = score*probabilities[j, label_combi[j]]\n",
    "        if score >= best_score:\n",
    "            best_score = score\n",
    "            best_combi = label_combi\n",
    "    return best_combi, score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main procedure for abductive Learning. Given a machine learning `model` and a prolog instance `pl` with `dataset`, it does the following steps:\n",
    "1. Using `model` to predict the pseudo-label probabilistic distribution of `dataset`;\n",
    "2. Finding the best pseudo-label combination considering both the abduction result from `pl` and the pseudo-label probabilistic distribution;\n",
    "3. Retrain the neural network with the abduced pseudo-labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ABL_main(model, pl, dataset, optimizer=None, scheduler=None):\n",
    "    # number of examples\n",
    "    num_examples = dataset.length\n",
    "    abduced_data_ids = [] # list of ids\n",
    "    abduced_labels = [] # list of labels\n",
    "    ground_truth_labels = [] # list of targets\n",
    "\n",
    "    # start abduction\n",
    "    for i in tqdm (range(num_examples), desc=\"Abducing...\"):\n",
    "        target = int(dataset.targets[i])\n",
    "        possible_pseudo_labels = abduce(pl, target) # given target, generate possible pseudo labels\n",
    "        if possible_pseudo_labels is not None:\n",
    "            # reshape the tensor of the two MNIST images to match NN model's input dimensions\n",
    "            img_indices = dataset.img_indices[i] # get the ids for the two images\n",
    "            imgs, _ = get_mnist_imgs(dataset1, img_indices, use_cuda=False) # get the two img tensors\n",
    "\n",
    "            pseudo_label_distribution = model(imgs).detach().numpy() # the Net\n",
    "\n",
    "            # find the pseudo-labels with the maximum likelihood\n",
    "            abduced_pseudo_labels, _ = best_pseudo_label(possible_pseudo_labels, pseudo_label_distribution)\n",
    "\n",
    "            # for abduced dataset\n",
    "            abduced_data_ids = abduced_data_ids + img_indices\n",
    "\n",
    "            abduced_labels = abduced_labels + abduced_pseudo_labels\n",
    "            ground_truth_labels = ground_truth_labels + dataset.ground_truth[i]\n",
    "\n",
    "    # changing the training data labels to the abduced labels\n",
    "    for i, img in enumerate(abduced_data_ids):\n",
    "        dataset1.targets[img] = abduced_labels[i]\n",
    "    \n",
    "    abduction_accuracy = np.sum(np.array(ground_truth_labels) == np.array(abduced_labels))/len(abduced_labels)\n",
    "\n",
    "\n",
    "    # making new dataset with abduced labels\n",
    "    abduced_data = torch.utils.data.Subset(dataset1, abduced_data_ids)\n",
    "\n",
    "    # training the neural network model\n",
    "    abduced_train_loader = torch.utils.data.DataLoader(abduced_data, batch_size=64)\n",
    "\n",
    "    for epoch in range(1, nn_epoch + 1):\n",
    "        train(model, device, abduced_train_loader, optimizer, epoch)\n",
    "        print(\"Abduction accuracy: \", abduction_accuracy)\n",
    "        scheduler.step()\n",
    "    test(model, device, nn_test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialise model and optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net(outdim=10).to(device)\n",
    "\n",
    "optimizer = optim.Adadelta(model.parameters(), lr=1.0)\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run Abductive Learning without any pre-train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Abducing...: 100%|██████████| 3000/3000 [00:21<00:00, 141.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6000 (0%)]\tLoss: 2.244581\n",
      "Abduction accuracy:  0.201\n",
      "Train Epoch: 2 [0/6000 (0%)]\tLoss: 2.010439\n",
      "Abduction accuracy:  0.201\n",
      "Train Epoch: 3 [0/6000 (0%)]\tLoss: 1.881090\n",
      "Abduction accuracy:  0.201\n",
      "Train Epoch: 4 [0/6000 (0%)]\tLoss: 1.899004\n",
      "Abduction accuracy:  0.201\n",
      "-- Test set: Average loss: 0.0322, Accuracy: 2294/10000 (23%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Abducing...: 100%|██████████| 3000/3000 [00:19<00:00, 156.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6000 (0%)]\tLoss: 1.561926\n",
      "Abduction accuracy:  0.3863333333333333\n",
      "Train Epoch: 2 [0/6000 (0%)]\tLoss: 1.186319\n",
      "Abduction accuracy:  0.3863333333333333\n",
      "Train Epoch: 3 [0/6000 (0%)]\tLoss: 1.226820\n",
      "Abduction accuracy:  0.3863333333333333\n",
      "Train Epoch: 4 [0/6000 (0%)]\tLoss: 1.236618\n",
      "Abduction accuracy:  0.3863333333333333\n",
      "-- Test set: Average loss: 0.0274, Accuracy: 4697/10000 (47%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Abducing...: 100%|██████████| 3000/3000 [00:30<00:00, 97.08it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6000 (0%)]\tLoss: 1.027938\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m ABL_epochs \u001b[39m=\u001b[39m \u001b[39m5\u001b[39m\n\u001b[0;32m      2\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(ABL_epochs):\n\u001b[1;32m----> 3\u001b[0m     ABL_main(model, prolog, mnist_sum_data_train, optimizer\u001b[39m=\u001b[39;49moptimizer, scheduler\u001b[39m=\u001b[39;49mscheduler)\n",
      "Cell \u001b[1;32mIn[21], line 42\u001b[0m, in \u001b[0;36mABL_main\u001b[1;34m(model, pl, dataset, optimizer, scheduler)\u001b[0m\n\u001b[0;32m     39\u001b[0m abduced_train_loader \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataLoader(abduced_data, batch_size\u001b[39m=\u001b[39m\u001b[39m64\u001b[39m)\n\u001b[0;32m     41\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, nn_epoch \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[1;32m---> 42\u001b[0m     train(model, device, abduced_train_loader, optimizer, epoch)\n\u001b[0;32m     43\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mAbduction accuracy: \u001b[39m\u001b[39m\"\u001b[39m, abduction_accuracy)\n\u001b[0;32m     44\u001b[0m     scheduler\u001b[39m.\u001b[39mstep()\n",
      "Cell \u001b[1;32mIn[5], line 112\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, device, train_loader, optimizer, epoch, log_interval, dry_run)\u001b[0m\n\u001b[0;32m    110\u001b[0m loss \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mloss_function(output, target)\n\u001b[0;32m    111\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m--> 112\u001b[0m optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[0;32m    113\u001b[0m \u001b[39mif\u001b[39;00m batch_idx \u001b[39m%\u001b[39m log_interval \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    114\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mTrain Epoch: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m [\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m (\u001b[39m\u001b[39m{:.0f}\u001b[39;00m\u001b[39m%\u001b[39m\u001b[39m)]\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39mLoss: \u001b[39m\u001b[39m{:.6f}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m    115\u001b[0m         epoch, batch_idx \u001b[39m*\u001b[39m \u001b[39mlen\u001b[39m(data), \u001b[39mlen\u001b[39m(train_loader\u001b[39m.\u001b[39mdataset),\n\u001b[0;32m    116\u001b[0m         \u001b[39m100.\u001b[39m \u001b[39m*\u001b[39m batch_idx \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(train_loader), loss\u001b[39m.\u001b[39mitem()))\n",
      "File \u001b[1;32mc:\\Users\\3908\\Desktop\\Research\\ABL_minimal\\.venv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:69\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m instance\u001b[39m.\u001b[39m_step_count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     68\u001b[0m wrapped \u001b[39m=\u001b[39m func\u001b[39m.\u001b[39m\u001b[39m__get__\u001b[39m(instance, \u001b[39mcls\u001b[39m)\n\u001b[1;32m---> 69\u001b[0m \u001b[39mreturn\u001b[39;00m wrapped(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\3908\\Desktop\\Research\\ABL_minimal\\.venv\\Lib\\site-packages\\torch\\optim\\optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    276\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    277\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m                                \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 280\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    281\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    283\u001b[0m \u001b[39m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\3908\\Desktop\\Research\\ABL_minimal\\.venv\\Lib\\site-packages\\torch\\optim\\optimizer.py:33\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     32\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m---> 33\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     34\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     35\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[1;32mc:\\Users\\3908\\Desktop\\Research\\ABL_minimal\\.venv\\Lib\\site-packages\\torch\\optim\\adadelta.py:108\u001b[0m, in \u001b[0;36mAdadelta.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m     96\u001b[0m     lr, rho, eps, weight_decay, foreach, maximize, differentiable \u001b[39m=\u001b[39m (\n\u001b[0;32m     97\u001b[0m         group[\u001b[39m\"\u001b[39m\u001b[39mlr\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m     98\u001b[0m         group[\u001b[39m\"\u001b[39m\u001b[39mrho\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    103\u001b[0m         group[\u001b[39m\"\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m    104\u001b[0m     )\n\u001b[0;32m    106\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_group(group, params_with_grad, grads, square_avgs, acc_deltas)\n\u001b[1;32m--> 108\u001b[0m     adadelta(\n\u001b[0;32m    109\u001b[0m         params_with_grad,\n\u001b[0;32m    110\u001b[0m         grads,\n\u001b[0;32m    111\u001b[0m         square_avgs,\n\u001b[0;32m    112\u001b[0m         acc_deltas,\n\u001b[0;32m    113\u001b[0m         lr\u001b[39m=\u001b[39;49mlr,\n\u001b[0;32m    114\u001b[0m         rho\u001b[39m=\u001b[39;49mrho,\n\u001b[0;32m    115\u001b[0m         eps\u001b[39m=\u001b[39;49meps,\n\u001b[0;32m    116\u001b[0m         weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[0;32m    117\u001b[0m         foreach\u001b[39m=\u001b[39;49mforeach,\n\u001b[0;32m    118\u001b[0m         maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[0;32m    119\u001b[0m         differentiable\u001b[39m=\u001b[39;49mdifferentiable,\n\u001b[0;32m    120\u001b[0m     )\n\u001b[0;32m    122\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Users\\3908\\Desktop\\Research\\ABL_minimal\\.venv\\Lib\\site-packages\\torch\\optim\\adadelta.py:206\u001b[0m, in \u001b[0;36madadelta\u001b[1;34m(params, grads, square_avgs, acc_deltas, foreach, differentiable, lr, rho, eps, weight_decay, maximize)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    204\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adadelta\n\u001b[1;32m--> 206\u001b[0m func(\n\u001b[0;32m    207\u001b[0m     params,\n\u001b[0;32m    208\u001b[0m     grads,\n\u001b[0;32m    209\u001b[0m     square_avgs,\n\u001b[0;32m    210\u001b[0m     acc_deltas,\n\u001b[0;32m    211\u001b[0m     lr\u001b[39m=\u001b[39;49mlr,\n\u001b[0;32m    212\u001b[0m     rho\u001b[39m=\u001b[39;49mrho,\n\u001b[0;32m    213\u001b[0m     eps\u001b[39m=\u001b[39;49meps,\n\u001b[0;32m    214\u001b[0m     weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[0;32m    215\u001b[0m     maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[0;32m    216\u001b[0m     differentiable\u001b[39m=\u001b[39;49mdifferentiable,\n\u001b[0;32m    217\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\3908\\Desktop\\Research\\ABL_minimal\\.venv\\Lib\\site-packages\\torch\\optim\\adadelta.py:249\u001b[0m, in \u001b[0;36m_single_tensor_adadelta\u001b[1;34m(params, grads, square_avgs, acc_deltas, lr, rho, eps, weight_decay, maximize, differentiable)\u001b[0m\n\u001b[0;32m    247\u001b[0m square_avg\u001b[39m.\u001b[39mmul_(rho)\u001b[39m.\u001b[39maddcmul_(grad, grad, value\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m rho)\n\u001b[0;32m    248\u001b[0m std \u001b[39m=\u001b[39m square_avg\u001b[39m.\u001b[39madd(eps)\u001b[39m.\u001b[39msqrt_()\n\u001b[1;32m--> 249\u001b[0m delta \u001b[39m=\u001b[39m acc_delta\u001b[39m.\u001b[39;49madd(eps)\u001b[39m.\u001b[39msqrt_()\n\u001b[0;32m    250\u001b[0m \u001b[39mif\u001b[39;00m differentiable:\n\u001b[0;32m    251\u001b[0m     delta \u001b[39m=\u001b[39m delta\u001b[39m.\u001b[39mclone()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ABL_epochs = 5\n",
    "for epoch in range(ABL_epochs):\n",
    "    ABL_main(model, prolog, mnist_sum_data_train, optimizer=optimizer, scheduler=scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Abducing...: 100%|██████████| 3000/3000 [00:32<00:00, 91.21it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6000 (0%)]\tLoss: 2.258973\n",
      "Abduction accuracy:  0.22833333333333333\n",
      "Train Epoch: 2 [0/6000 (0%)]\tLoss: 2.106419\n",
      "Abduction accuracy:  0.22833333333333333\n",
      "-- Test set: Average loss: 0.0327, Accuracy: 2969/10000 (30%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Abducing...: 100%|██████████| 3000/3000 [00:31<00:00, 95.85it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6000 (0%)]\tLoss: 1.802510\n",
      "Abduction accuracy:  0.438\n",
      "Train Epoch: 2 [0/6000 (0%)]\tLoss: 1.359203\n",
      "Abduction accuracy:  0.438\n",
      "-- Test set: Average loss: 0.0220, Accuracy: 4786/10000 (48%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Abducing...: 100%|██████████| 3000/3000 [00:28<00:00, 106.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6000 (0%)]\tLoss: 1.242864\n",
      "Abduction accuracy:  0.6496666666666666\n",
      "Train Epoch: 2 [0/6000 (0%)]\tLoss: 1.289982\n",
      "Abduction accuracy:  0.6496666666666666\n",
      "-- Test set: Average loss: 0.0124, Accuracy: 7156/10000 (72%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Abducing...: 100%|██████████| 3000/3000 [00:31<00:00, 94.73it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6000 (0%)]\tLoss: 0.956407\n",
      "Abduction accuracy:  0.8726666666666667\n",
      "Train Epoch: 2 [0/6000 (0%)]\tLoss: 0.751012\n",
      "Abduction accuracy:  0.8726666666666667\n",
      "-- Test set: Average loss: 0.0057, Accuracy: 9076/10000 (91%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Abducing...: 100%|██████████| 3000/3000 [00:32<00:00, 92.40it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6000 (0%)]\tLoss: 0.592407\n",
      "Abduction accuracy:  0.9776666666666667\n",
      "Train Epoch: 2 [0/6000 (0%)]\tLoss: 0.539589\n",
      "Abduction accuracy:  0.9776666666666667\n",
      "-- Test set: Average loss: 0.0033, Accuracy: 9448/10000 (94%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Abducing...: 100%|██████████| 3000/3000 [00:32<00:00, 93.57it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6000 (0%)]\tLoss: 0.362854\n",
      "Abduction accuracy:  0.9923333333333333\n",
      "Train Epoch: 2 [0/6000 (0%)]\tLoss: 0.447820\n",
      "Abduction accuracy:  0.9923333333333333\n",
      "-- Test set: Average loss: 0.0029, Accuracy: 9531/10000 (95%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Abducing...: 100%|██████████| 3000/3000 [00:31<00:00, 95.28it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6000 (0%)]\tLoss: 0.480746\n",
      "Abduction accuracy:  0.9943333333333333\n",
      "Train Epoch: 2 [0/6000 (0%)]\tLoss: 0.360771\n",
      "Abduction accuracy:  0.9943333333333333\n",
      "-- Test set: Average loss: 0.0027, Accuracy: 9558/10000 (96%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Abducing...: 100%|██████████| 3000/3000 [00:33<00:00, 90.45it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6000 (0%)]\tLoss: 0.297648\n",
      "Abduction accuracy:  0.995\n",
      "Train Epoch: 2 [0/6000 (0%)]\tLoss: 0.457465\n",
      "Abduction accuracy:  0.995\n",
      "-- Test set: Average loss: 0.0027, Accuracy: 9562/10000 (96%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Abducing...: 100%|██████████| 3000/3000 [00:32<00:00, 91.29it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6000 (0%)]\tLoss: 0.478222\n",
      "Abduction accuracy:  0.9953333333333333\n",
      "Train Epoch: 2 [0/6000 (0%)]\tLoss: 0.415555\n",
      "Abduction accuracy:  0.9953333333333333\n",
      "-- Test set: Average loss: 0.0027, Accuracy: 9567/10000 (96%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Abducing...: 100%|██████████| 3000/3000 [00:30<00:00, 99.86it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6000 (0%)]\tLoss: 0.388577\n",
      "Abduction accuracy:  0.9953333333333333\n",
      "Train Epoch: 2 [0/6000 (0%)]\tLoss: 0.490199\n",
      "Abduction accuracy:  0.9953333333333333\n",
      "-- Test set: Average loss: 0.0026, Accuracy: 9566/10000 (96%)\n"
     ]
    }
   ],
   "source": [
    "ABL_epochs = 5\n",
    "for epoch in range(ABL_epochs):\n",
    "    ABL_main(model, prolog, mnist_sum_data_train, optimizer=optimizer, scheduler=scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run Abductive Learning with one-shot pre-train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample an one-shot training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/10 (0%)]\tLoss: 2.337119\n",
      "Train Epoch: 2 [0/10 (0%)]\tLoss: 2.096287\n",
      "Train Epoch: 3 [0/10 (0%)]\tLoss: 1.946862\n",
      "Train Epoch: 4 [0/10 (0%)]\tLoss: 1.636731\n",
      "-- Test set: Average loss: 0.0306, Accuracy: 3867/10000 (39%)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# reset the machine learning model\n",
    "model = Net(outdim=10).to(device)\n",
    "\n",
    "# reset dataset1 to reset the labels for one-shot training, \n",
    "# since the previous abductive learning process has changed \n",
    "# the ground truth labels in dataset1\n",
    "dataset1 = datasets.MNIST('data', train=True, download=True,\n",
    "                            transform=transform)\n",
    "\n",
    "n_samples = 1\n",
    "few_shot_indices = []\n",
    "\n",
    "for i in range(10):\n",
    "    few_shot_indices = few_shot_indices + \\\n",
    "        random.sample(digit_groups_train[i], n_samples)\n",
    "\n",
    "# few_shot_indices = random.sample(all_img_indices, n_samples)\n",
    "\n",
    "sup_imgs_train = torch.utils.data.Subset(dataset1, few_shot_indices)\n",
    "\n",
    "sup_train_loader = torch.utils.data.DataLoader(\n",
    "    sup_imgs_train, **nn_train_kwargs)\n",
    "\n",
    "optimizer = optim.Adadelta(model.parameters(), lr=1.0)\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=0.7)\n",
    "\n",
    "for epoch in range(1, 5):\n",
    "    train(model, device, sup_train_loader,\n",
    "        optimizer, epoch)\n",
    "    #test(model, device, nn_test_loader)\n",
    "    scheduler.step()\n",
    "test(model, device, nn_test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Abducing...: 100%|██████████| 3000/3000 [00:30<00:00, 97.97it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6000 (0%)]\tLoss: 1.958068\n",
      "Abduction accuracy:  0.6456666666666667\n",
      "Train Epoch: 2 [0/6000 (0%)]\tLoss: 1.296085\n",
      "Abduction accuracy:  0.6456666666666667\n",
      "-- Test set: Average loss: 0.0137, Accuracy: 7669/10000 (77%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Abducing...: 100%|██████████| 3000/3000 [00:29<00:00, 101.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6000 (0%)]\tLoss: 0.926988\n",
      "Abduction accuracy:  0.9103333333333333\n",
      "Train Epoch: 2 [0/6000 (0%)]\tLoss: 0.585736\n",
      "Abduction accuracy:  0.9103333333333333\n",
      "-- Test set: Average loss: 0.0046, Accuracy: 9210/10000 (92%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Abducing...: 100%|██████████| 3000/3000 [00:30<00:00, 99.83it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6000 (0%)]\tLoss: 0.608485\n",
      "Abduction accuracy:  0.9873333333333333\n",
      "Train Epoch: 2 [0/6000 (0%)]\tLoss: 0.344015\n",
      "Abduction accuracy:  0.9873333333333333\n",
      "-- Test set: Average loss: 0.0027, Accuracy: 9508/10000 (95%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Abducing...: 100%|██████████| 3000/3000 [00:29<00:00, 101.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6000 (0%)]\tLoss: 0.331251\n",
      "Abduction accuracy:  0.995\n",
      "Train Epoch: 2 [0/6000 (0%)]\tLoss: 0.349095\n",
      "Abduction accuracy:  0.995\n",
      "-- Test set: Average loss: 0.0024, Accuracy: 9554/10000 (96%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Abducing...: 100%|██████████| 3000/3000 [00:29<00:00, 100.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6000 (0%)]\tLoss: 0.333204\n",
      "Abduction accuracy:  0.9953333333333333\n",
      "Train Epoch: 2 [0/6000 (0%)]\tLoss: 0.350591\n",
      "Abduction accuracy:  0.9953333333333333\n",
      "-- Test set: Average loss: 0.0023, Accuracy: 9561/10000 (96%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Abducing...: 100%|██████████| 3000/3000 [00:29<00:00, 101.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6000 (0%)]\tLoss: 0.193948\n",
      "Abduction accuracy:  0.9956666666666667\n",
      "Train Epoch: 2 [0/6000 (0%)]\tLoss: 0.240049\n",
      "Abduction accuracy:  0.9956666666666667\n",
      "-- Test set: Average loss: 0.0023, Accuracy: 9576/10000 (96%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Abducing...: 100%|██████████| 3000/3000 [00:29<00:00, 101.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6000 (0%)]\tLoss: 0.218380\n",
      "Abduction accuracy:  0.9956666666666667\n",
      "Train Epoch: 2 [0/6000 (0%)]\tLoss: 0.282037\n",
      "Abduction accuracy:  0.9956666666666667\n",
      "-- Test set: Average loss: 0.0022, Accuracy: 9574/10000 (96%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Abducing...: 100%|██████████| 3000/3000 [00:29<00:00, 101.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6000 (0%)]\tLoss: 0.307142\n",
      "Abduction accuracy:  0.9956666666666667\n",
      "Train Epoch: 2 [0/6000 (0%)]\tLoss: 0.427967\n",
      "Abduction accuracy:  0.9956666666666667\n",
      "-- Test set: Average loss: 0.0022, Accuracy: 9578/10000 (96%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Abducing...: 100%|██████████| 3000/3000 [00:29<00:00, 101.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6000 (0%)]\tLoss: 0.305546\n",
      "Abduction accuracy:  0.9956666666666667\n",
      "Train Epoch: 2 [0/6000 (0%)]\tLoss: 0.310958\n",
      "Abduction accuracy:  0.9956666666666667\n",
      "-- Test set: Average loss: 0.0022, Accuracy: 9580/10000 (96%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Abducing...: 100%|██████████| 3000/3000 [00:29<00:00, 102.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6000 (0%)]\tLoss: 0.294766\n",
      "Abduction accuracy:  0.9956666666666667\n",
      "Train Epoch: 2 [0/6000 (0%)]\tLoss: 0.306095\n",
      "Abduction accuracy:  0.9956666666666667\n",
      "-- Test set: Average loss: 0.0022, Accuracy: 9579/10000 (96%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Abducing...: 100%|██████████| 3000/3000 [00:29<00:00, 100.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6000 (0%)]\tLoss: 0.367064\n",
      "Abduction accuracy:  0.9956666666666667\n",
      "Train Epoch: 2 [0/6000 (0%)]\tLoss: 0.403212\n",
      "Abduction accuracy:  0.9956666666666667\n",
      "-- Test set: Average loss: 0.0022, Accuracy: 9579/10000 (96%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Abducing...: 100%|██████████| 3000/3000 [00:29<00:00, 100.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6000 (0%)]\tLoss: 0.338030\n",
      "Abduction accuracy:  0.9956666666666667\n",
      "Train Epoch: 2 [0/6000 (0%)]\tLoss: 0.229107\n",
      "Abduction accuracy:  0.9956666666666667\n",
      "-- Test set: Average loss: 0.0022, Accuracy: 9579/10000 (96%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Abducing...: 100%|██████████| 3000/3000 [00:30<00:00, 99.94it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6000 (0%)]\tLoss: 0.222177\n",
      "Abduction accuracy:  0.9956666666666667\n",
      "Train Epoch: 2 [0/6000 (0%)]\tLoss: 0.232248\n",
      "Abduction accuracy:  0.9956666666666667\n",
      "-- Test set: Average loss: 0.0022, Accuracy: 9579/10000 (96%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Abducing...: 100%|██████████| 3000/3000 [00:29<00:00, 100.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6000 (0%)]\tLoss: 0.355552\n",
      "Abduction accuracy:  0.9956666666666667\n",
      "Train Epoch: 2 [0/6000 (0%)]\tLoss: 0.440173\n",
      "Abduction accuracy:  0.9956666666666667\n",
      "-- Test set: Average loss: 0.0022, Accuracy: 9579/10000 (96%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Abducing...: 100%|██████████| 3000/3000 [00:30<00:00, 99.89it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6000 (0%)]\tLoss: 0.301352\n",
      "Abduction accuracy:  0.9956666666666667\n",
      "Train Epoch: 2 [0/6000 (0%)]\tLoss: 0.249726\n",
      "Abduction accuracy:  0.9956666666666667\n",
      "-- Test set: Average loss: 0.0022, Accuracy: 9579/10000 (96%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Abducing...: 100%|██████████| 3000/3000 [00:20<00:00, 145.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6000 (0%)]\tLoss: 0.252311\n",
      "Abduction accuracy:  0.9956666666666667\n",
      "Train Epoch: 2 [0/6000 (0%)]\tLoss: 0.270329\n",
      "Abduction accuracy:  0.9956666666666667\n",
      "-- Test set: Average loss: 0.0023, Accuracy: 9579/10000 (96%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Abducing...: 100%|██████████| 3000/3000 [00:26<00:00, 114.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6000 (0%)]\tLoss: 0.370963\n",
      "Abduction accuracy:  0.9956666666666667\n",
      "Train Epoch: 2 [0/6000 (0%)]\tLoss: 0.205726\n",
      "Abduction accuracy:  0.9956666666666667\n",
      "-- Test set: Average loss: 0.0022, Accuracy: 9579/10000 (96%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Abducing...: 100%|██████████| 3000/3000 [00:28<00:00, 104.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6000 (0%)]\tLoss: 0.236914\n",
      "Abduction accuracy:  0.9956666666666667\n",
      "Train Epoch: 2 [0/6000 (0%)]\tLoss: 0.317033\n",
      "Abduction accuracy:  0.9956666666666667\n",
      "-- Test set: Average loss: 0.0022, Accuracy: 9579/10000 (96%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Abducing...: 100%|██████████| 3000/3000 [00:19<00:00, 152.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6000 (0%)]\tLoss: 0.265046\n",
      "Abduction accuracy:  0.9956666666666667\n",
      "Train Epoch: 2 [0/6000 (0%)]\tLoss: 0.282606\n",
      "Abduction accuracy:  0.9956666666666667\n",
      "-- Test set: Average loss: 0.0022, Accuracy: 9579/10000 (96%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Abducing...: 100%|██████████| 3000/3000 [00:17<00:00, 170.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6000 (0%)]\tLoss: 0.353039\n",
      "Abduction accuracy:  0.9956666666666667\n",
      "Train Epoch: 2 [0/6000 (0%)]\tLoss: 0.300775\n",
      "Abduction accuracy:  0.9956666666666667\n",
      "-- Test set: Average loss: 0.0022, Accuracy: 9579/10000 (96%)\n"
     ]
    }
   ],
   "source": [
    "ABL_epochs = 20\n",
    "for epoch in range(ABL_epochs):\n",
    "    ABL_main(model, prolog, mnist_sum_data_train, optimizer=optimizer, scheduler=scheduler)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0818a47565ceb6402158bbf0dc496aa25facf52e7106c9c4cd712582e3748063"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
