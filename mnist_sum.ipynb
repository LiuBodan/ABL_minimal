{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import `pyswip` and consult the Prolog background knowledge base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 adds 9 equals 10.\n",
      "2 adds 8 equals 10.\n",
      "3 adds 7 equals 10.\n",
      "4 adds 6 equals 10.\n",
      "5 adds 5 equals 10.\n",
      "6 adds 4 equals 10.\n",
      "7 adds 3 equals 10.\n",
      "8 adds 2 equals 10.\n",
      "9 adds 1 equals 10.\n"
     ]
    }
   ],
   "source": [
    "from pyswip import Prolog\n",
    "prolog = Prolog()\n",
    "prolog.consult('mnist_sum.pl')\n",
    "target = 10\n",
    "for soln in prolog.query(\"abduce([X,Y], {})\".format(target)):\n",
    "    print(soln[\"X\"], \"adds\", soln[\"Y\"], \"equals {}.\".format(target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 adds 9 equals 10.\n",
      "2 adds 8 equals 10.\n",
      "3 adds 7 equals 10.\n",
      "4 adds 6 equals 10.\n",
      "5 adds 5 equals 10.\n",
      "6 adds 4 equals 10.\n",
      "7 adds 3 equals 10.\n",
      "8 adds 2 equals 10.\n",
      "9 adds 1 equals 10.\n"
     ]
    }
   ],
   "source": [
    "from pyswip import Prolog\n",
    "prolog = Prolog()\n",
    "prolog.consult('paraconsistent.pl')\n",
    "target = 10\n",
    "for soln in prolog.query(\"abduce([X,Y], {})\".format(target)):\n",
    "    print(soln[\"X\"], \"adds\", soln[\"Y\"], \"equals {}.\".format(target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test if the `CLP(FD)`-based abduction in the background knowledge base works correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abductive Learning\n",
    "\n",
    "Now let's try to implement the MNIST sum learning algorithm using the Abductive Learning framework.\n",
    "\n",
    "### Dataset Generation\n",
    "\n",
    "Directly copy the codes from the `data_generator.ipynb` notebook file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "transform=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "dataset1 = datasets.MNIST('data', train=True, download=True,\n",
    "                            transform=transform)\n",
    "dataset2 = datasets.MNIST('data', train=False,\n",
    "                            transform=transform)\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "digit_groups_train = {0:[], 1:[], 2:[], 3:[], 4:[], 5:[], 6:[], 7:[], 8:[], 9:[]}\n",
    "digit_groups_test = {0:[], 1:[], 2:[], 3:[], 4:[], 5:[], 6:[], 7:[], 8:[], 9:[]}\n",
    "\n",
    "for i in range(len(dataset1)): \n",
    "    digit_groups_train[int(dataset1.targets[i])].append(i)\n",
    "for i in range(len(dataset2)): \n",
    "    digit_groups_test[int(dataset2.targets[i])].append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_Sum:\n",
    "    def __init__(self, num, digit_groups):\n",
    "        self.targets = []\n",
    "        self.img_indices = [] # list of lists of 2 ids\n",
    "        self.ground_truth = []\n",
    "        self.length = num\n",
    "        for i in range(num):\n",
    "            # sampling two numbers from 0 to 9\n",
    "            sampled_digits = np.random.choice(10, 2)\n",
    "            self.ground_truth.append(list(sampled_digits))\n",
    "\n",
    "            # using the sum of the sampled digits as the target\n",
    "            self.targets.append(sum(sampled_digits))\n",
    "            ids = []\n",
    "            for j in range(len(sampled_digits)):\n",
    "                # get the j-th digits\n",
    "                digit = sampled_digits[j]\n",
    "                # total number of the images of the digit\n",
    "                ids.append(np.random.choice(digit_groups[digit]))\n",
    "            self.img_indices.append(ids)\n",
    "\n",
    "# Generate the training and test dataset for MNIST Sum task\n",
    "mnist_sum_data_train = MNIST_Sum(3000, digit_groups_train)\n",
    "mnist_sum_data_test = MNIST_Sum(3000, digit_groups_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Machine Learning Part\n",
    "\n",
    "Neural networks for image classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)\n",
    "\n",
    "\n",
    "def conv_net(outdim, *args, **kwargs):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(1, 32, 3, 1),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(32, 64, 3, 1),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(2),\n",
    "        nn.Dropout(0.25),\n",
    "        Flatten(),\n",
    "        nn.Linear(9216, 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.5),\n",
    "        nn.Linear(128, outdim),\n",
    "        nn.LogSoftmax(dim=1)\n",
    "    )\n",
    "\n",
    "\n",
    "def auto_enc(outdim, *args, **kwargs):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(outdim, 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128, 784)\n",
    "    )\n",
    "\n",
    "\n",
    "def mlp(indim, outdim, *args, **kwargs):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(indim, 32),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(32, outdim),\n",
    "        nn.LogSoftmax(dim=1),\n",
    "    )\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    \"\"\"A (Bi)LSTM Model.\n",
    "\n",
    "    Attributes:\n",
    "        num_layers: the number of LSTM layers (number of stacked LSTM models) in the network.\n",
    "        in_dim: the size of the input sample.\n",
    "        hidden_dim: the size of the hidden layers.\n",
    "        out_dim: the size of the output.\n",
    "        activation: the activation function.\n",
    "        bidirectional: the flag for bidirectional LSTM\n",
    "        dropout: the dropout rate if num_layers > 1\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_layers, in_dim, hidden_dim, out_dim,\n",
    "                 bidirectional=False, dropout=0):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.in_dim = in_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.bidirectional = bidirectional\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.lstm = nn.LSTM(self.in_dim,\n",
    "                            self.hidden_dim,\n",
    "                            num_layers=self.num_layers,\n",
    "                            bidirectional=self.bidirectional,\n",
    "                            dropout=self.dropout,\n",
    "                            batch_first=True)\n",
    "        fc_dim = self.hidden_dim * 2 if self.bidirectional else self.hidden_dim\n",
    "        self.fc = nn.Linear(fc_dim, self.out_dim)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        lstm_out, _ = self.lstm(inputs)\n",
    "        outputs = self.fc(lstm_out[:, -1, :])\n",
    "        outputs = torch.sigmoid(outputs)\n",
    "        return outputs\n",
    "\n",
    "    def loss_function(self, pred, y):\n",
    "        return F.binary_cross_entropy(pred, y.view(y.shape[0], -1))\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    outdim = 10\n",
    "\n",
    "    def __init__(self, outdim):\n",
    "        super(Net, self).__init__()\n",
    "        self.outdim = outdim\n",
    "        self.enc = conv_net(outdim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.enc(x)\n",
    "        return output\n",
    "\n",
    "    def loss_function(self, pred, y):\n",
    "        return F.nll_loss(pred, y)\n",
    "\n",
    "def train(model, device, train_loader, optimizer, epoch,\n",
    "          log_interval=1000, dry_run=False):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = model.loss_function(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "            if dry_run:\n",
    "                break\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            # sum up batch loss\n",
    "            test_loss += model.loss_function(output, target).item()\n",
    "            # get the index of the max log-probability\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('-- Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Logic Abduction Part\n",
    "\n",
    "It involves the following steps:\n",
    "1. Given the target (label, i.e., the sum of the two images), using `pyswip` to abduce possible pseudo-labels for them.\n",
    "2. Calculate the probability of each pair of pseudo-labels.\n",
    "3. Return the most probable pseudo-labels to retrain the neural network.\n",
    "\n",
    "_Remark_: For more complicated problems, a better way of searching for the best pseudo-labels is required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Abducing possible pseudo-labels given the sum\n",
    "\n",
    "- `pl` is the Prolog instance that consulted `mnist_sum.pl`;\n",
    "- `target` is the sum of two images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def abduce(pl, target):\n",
    "    # This abduce/2 function is defined in \"mnist_sum.pl\"\n",
    "    ans = [];\n",
    "    for soln in pl.query(\"abduce([X,Y], {})\".format(target)): # given targer, abduce X and Y\n",
    "        ans.append([soln[\"X\"], soln[\"Y\"]])\n",
    "    if len(ans) > 0:\n",
    "        return ans\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the `abduce` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 5], [1, 4], [2, 3], [3, 2], [4, 1], [5, 0]]\n"
     ]
    }
   ],
   "source": [
    "print(abduce(prolog, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The Abductive Learning Procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing useful libraries and set the default neural network training parameters within the Abductive Learning Process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "nn_train_kwargs = {'batch_size': 64, 'shuffle': True}\n",
    "nn_epoch = 2\n",
    "\n",
    "\n",
    "nn_test_loader = torch.utils.data.DataLoader(dataset2, **nn_train_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful functions for abduction:\n",
    "1. `get_mnist_imgs`: given `indices`, sample a subset of images from `dataset` (such as the `MNIST` dataset).\n",
    "2. `best_pseudo_label`: given a set of abduced possible pseudo-labels and the pseudo-label distribution, return the most probable pseudo-label combination for each image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mnist_imgs(dataset, indices, use_cuda=False):\n",
    "    \"\"\"\n",
    "    Get the image tensor from mnist dataset by indices\n",
    "    \"\"\"\n",
    "\n",
    "    n = len(indices)\n",
    "    img_tensor, tgt = dataset[indices[0]]\n",
    "    img_tensor = torch.reshape(img_tensor, (1, 1, 28, 28))\n",
    "    targets = [tgt]\n",
    "    i = 1\n",
    "    while i < n:\n",
    "        img, tgt = dataset[indices[i]]\n",
    "        img = torch.reshape(img, (1, 1, 28, 28))\n",
    "        img_tensor = torch.cat((img_tensor, img), 0)\n",
    "        targets.append(tgt)\n",
    "        i = i + 1\n",
    "    if use_cuda:\n",
    "        img_tensor = img_tensor.to(torch.device(\"cuda\"))\n",
    "    return img_tensor, targets # list of image tensors and list of its targets\n",
    "\n",
    "def best_pseudo_label(pseudo_label_lists, pseudo_label_dist):\n",
    "    \"\"\"\n",
    "    Given plabel lists and plabel distribution list\n",
    "    Return the best combination with score\n",
    "    \"\"\"\n",
    "    best_score = -100000.0\n",
    "    best_combi = np.zeros(pseudo_label_dist.shape[0])\n",
    "    probabilities = np.exp(pseudo_label_dist)\n",
    "    for label_combi in pseudo_label_lists:\n",
    "        # because the scores are log_softmax, the log probability can be calculated as sum\n",
    "        score = 1.0\n",
    "        for j in range(len(label_combi)):\n",
    "            score = score*probabilities[j, label_combi[j]]\n",
    "        if score >= best_score:\n",
    "            best_score = score\n",
    "            best_combi = label_combi\n",
    "    return best_combi, score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main procedure for abductive Learning. Given a machine learning `model` and a prolog instance `pl` with `dataset`, it does the following steps:\n",
    "1. Using `model` to predict the pseudo-label probabilistic distribution of `dataset`;\n",
    "2. Finding the best pseudo-label combination considering both the abduction result from `pl` and the pseudo-label probabilistic distribution;\n",
    "3. Retrain the neural network with the abduced pseudo-labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ABL_main(model, pl, dataset, optimizer=None, scheduler=None):\n",
    "    # number of examples\n",
    "    num_examples = dataset.length\n",
    "    abduced_data_ids = [] # list of ids\n",
    "    abduced_labels = [] # list of labels\n",
    "    ground_truth_labels = [] # list of targets\n",
    "\n",
    "    # start abduction\n",
    "    for i in tqdm (range(num_examples), desc=\"Abducing...\"):\n",
    "        target = int(dataset.targets[i])\n",
    "        possible_pseudo_labels = abduce(pl, target) # given target, generate possible pseudo labels\n",
    "        if possible_pseudo_labels is not None:\n",
    "            # reshape the tensor of the two MNIST images to match NN model's input dimensions\n",
    "            img_indices = dataset.img_indices[i] # get the ids for the two images\n",
    "            imgs, _ = get_mnist_imgs(dataset1, img_indices, use_cuda=False) # get the two img tensors\n",
    "\n",
    "            pseudo_label_distribution = model(imgs).detach().numpy() # the Net\n",
    "\n",
    "            # find the pseudo-labels with the maximum likelihood\n",
    "            abduced_pseudo_labels, _ = best_pseudo_label(possible_pseudo_labels, pseudo_label_distribution)\n",
    "\n",
    "            # for abduced dataset\n",
    "            abduced_data_ids = abduced_data_ids + img_indices\n",
    "\n",
    "            abduced_labels = abduced_labels + abduced_pseudo_labels\n",
    "            ground_truth_labels = ground_truth_labels + dataset.ground_truth[i]\n",
    "\n",
    "    # changing the training data labels to the abduced labels\n",
    "    for i, img in enumerate(abduced_data_ids):\n",
    "        dataset1.targets[img] = abduced_labels[i]\n",
    "    \n",
    "    abduction_accuracy = np.sum(np.array(ground_truth_labels) == np.array(abduced_labels))/len(abduced_labels)\n",
    "\n",
    "\n",
    "    # making new dataset with abduced labels\n",
    "    abduced_data = torch.utils.data.Subset(dataset1, abduced_data_ids)\n",
    "\n",
    "    # training the neural network model\n",
    "    abduced_train_loader = torch.utils.data.DataLoader(abduced_data, batch_size=64)\n",
    "    \n",
    "\n",
    "    for epoch in range(1, nn_epoch + 1):\n",
    "        train(model, device, abduced_train_loader, optimizer, epoch)\n",
    "        print(\"Abduction accuracy: \", abduction_accuracy)\n",
    "        scheduler.step()\n",
    "    test(model, device, nn_test_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialise model and optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_sum_data_train.targets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4],\n",
      "        [4]])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.eval()\n",
    "img_indices = mnist_sum_data_train.img_indices[0] # get the ids for the two images\n",
    "imgs, _ = get_mnist_imgs(dataset1, img_indices, use_cuda=False) # get the two img tensors\n",
    "# Plot them\n",
    "_, axs = plt.subplots(1,2)\n",
    "axs[0].imshow(dataset1.data[img_indices[0], :, :])\n",
    "axs[1].imshow(dataset1.data[img_indices[1], :, ])\n",
    "pseudo_label_distribution = model(imgs).detach().numpy() # the Net\n",
    "\n",
    "\n",
    "print(model(imgs).argmax(dim=1, keepdim=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 5]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "possible_pseudo_labels = abduce(prolog, 6) # given target, generate possible pseudo labels\n",
    "pseudo_label_distribution = model(imgs).detach().numpy() # the Net\n",
    "abduced_pseudo_labels, _ = best_pseudo_label(possible_pseudo_labels, pseudo_label_distribution)\n",
    "abduced_pseudo_labels\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net(outdim=10).to(device)\n",
    "\n",
    "optimizer = optim.Adadelta(model.parameters(), lr=1.0)\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=0.7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn_epoch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, nn_epoch \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[0;32m      2\u001b[0m         normaltrain \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataLoader(dataset1, batch_size\u001b[39m=\u001b[39m\u001b[39m64\u001b[39m)\n\u001b[0;32m      3\u001b[0m         train(model, device, normaltrain, optimizer, epoch)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nn_epoch' is not defined"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, nn_epoch + 1):\n",
    "        normaltrain = torch.utils.data.DataLoader(dataset1, batch_size=64)\n",
    "        train(model, device, normaltrain, optimizer, epoch)\n",
    "        scheduler.step()\n",
    "        test(model, device, nn_test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run Abductive Learning without any pre-train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ABL_epochs = 5\n",
    "for epoch in range(ABL_epochs):\n",
    "    ABL_main(model, prolog, mnist_sum_data_train, optimizer=optimizer, scheduler=scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Abducing...: 100%|██████████| 3000/3000 [00:32<00:00, 91.21it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6000 (0%)]\tLoss: 2.258973\n",
      "Abduction accuracy:  0.22833333333333333\n",
      "Train Epoch: 2 [0/6000 (0%)]\tLoss: 2.106419\n",
      "Abduction accuracy:  0.22833333333333333\n",
      "-- Test set: Average loss: 0.0327, Accuracy: 2969/10000 (30%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Abducing...: 100%|██████████| 3000/3000 [00:31<00:00, 95.85it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6000 (0%)]\tLoss: 1.802510\n",
      "Abduction accuracy:  0.438\n",
      "Train Epoch: 2 [0/6000 (0%)]\tLoss: 1.359203\n",
      "Abduction accuracy:  0.438\n",
      "-- Test set: Average loss: 0.0220, Accuracy: 4786/10000 (48%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Abducing...: 100%|██████████| 3000/3000 [00:28<00:00, 106.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6000 (0%)]\tLoss: 1.242864\n",
      "Abduction accuracy:  0.6496666666666666\n",
      "Train Epoch: 2 [0/6000 (0%)]\tLoss: 1.289982\n",
      "Abduction accuracy:  0.6496666666666666\n",
      "-- Test set: Average loss: 0.0124, Accuracy: 7156/10000 (72%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Abducing...: 100%|██████████| 3000/3000 [00:31<00:00, 94.73it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6000 (0%)]\tLoss: 0.956407\n",
      "Abduction accuracy:  0.8726666666666667\n",
      "Train Epoch: 2 [0/6000 (0%)]\tLoss: 0.751012\n",
      "Abduction accuracy:  0.8726666666666667\n",
      "-- Test set: Average loss: 0.0057, Accuracy: 9076/10000 (91%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Abducing...: 100%|██████████| 3000/3000 [00:32<00:00, 92.40it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6000 (0%)]\tLoss: 0.592407\n",
      "Abduction accuracy:  0.9776666666666667\n",
      "Train Epoch: 2 [0/6000 (0%)]\tLoss: 0.539589\n",
      "Abduction accuracy:  0.9776666666666667\n",
      "-- Test set: Average loss: 0.0033, Accuracy: 9448/10000 (94%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Abducing...: 100%|██████████| 3000/3000 [00:32<00:00, 93.57it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6000 (0%)]\tLoss: 0.362854\n",
      "Abduction accuracy:  0.9923333333333333\n",
      "Train Epoch: 2 [0/6000 (0%)]\tLoss: 0.447820\n",
      "Abduction accuracy:  0.9923333333333333\n",
      "-- Test set: Average loss: 0.0029, Accuracy: 9531/10000 (95%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Abducing...: 100%|██████████| 3000/3000 [00:31<00:00, 95.28it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6000 (0%)]\tLoss: 0.480746\n",
      "Abduction accuracy:  0.9943333333333333\n",
      "Train Epoch: 2 [0/6000 (0%)]\tLoss: 0.360771\n",
      "Abduction accuracy:  0.9943333333333333\n",
      "-- Test set: Average loss: 0.0027, Accuracy: 9558/10000 (96%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Abducing...: 100%|██████████| 3000/3000 [00:33<00:00, 90.45it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6000 (0%)]\tLoss: 0.297648\n",
      "Abduction accuracy:  0.995\n",
      "Train Epoch: 2 [0/6000 (0%)]\tLoss: 0.457465\n",
      "Abduction accuracy:  0.995\n",
      "-- Test set: Average loss: 0.0027, Accuracy: 9562/10000 (96%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Abducing...: 100%|██████████| 3000/3000 [00:32<00:00, 91.29it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6000 (0%)]\tLoss: 0.478222\n",
      "Abduction accuracy:  0.9953333333333333\n",
      "Train Epoch: 2 [0/6000 (0%)]\tLoss: 0.415555\n",
      "Abduction accuracy:  0.9953333333333333\n",
      "-- Test set: Average loss: 0.0027, Accuracy: 9567/10000 (96%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Abducing...: 100%|██████████| 3000/3000 [00:30<00:00, 99.86it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6000 (0%)]\tLoss: 0.388577\n",
      "Abduction accuracy:  0.9953333333333333\n",
      "Train Epoch: 2 [0/6000 (0%)]\tLoss: 0.490199\n",
      "Abduction accuracy:  0.9953333333333333\n",
      "-- Test set: Average loss: 0.0026, Accuracy: 9566/10000 (96%)\n"
     ]
    }
   ],
   "source": [
    "ABL_epochs = 5\n",
    "for epoch in range(ABL_epochs):\n",
    "    ABL_main(model, prolog, mnist_sum_data_train, optimizer=optimizer, scheduler=scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run Abductive Learning with one-shot pre-train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample an one-shot training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/10 (0%)]\tLoss: 2.337119\n",
      "Train Epoch: 2 [0/10 (0%)]\tLoss: 2.096287\n",
      "Train Epoch: 3 [0/10 (0%)]\tLoss: 1.946862\n",
      "Train Epoch: 4 [0/10 (0%)]\tLoss: 1.636731\n",
      "-- Test set: Average loss: 0.0306, Accuracy: 3867/10000 (39%)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# reset the machine learning model\n",
    "model = Net(outdim=10).to(device)\n",
    "\n",
    "# reset dataset1 to reset the labels for one-shot training, \n",
    "# since the previous abductive learning process has changed \n",
    "# the ground truth labels in dataset1\n",
    "dataset1 = datasets.MNIST('data', train=True, download=True,\n",
    "                            transform=transform)\n",
    "\n",
    "n_samples = 1\n",
    "few_shot_indices = []\n",
    "\n",
    "for i in range(10):\n",
    "    few_shot_indices = few_shot_indices + \\\n",
    "        random.sample(digit_groups_train[i], n_samples)\n",
    "\n",
    "# few_shot_indices = random.sample(all_img_indices, n_samples)\n",
    "\n",
    "sup_imgs_train = torch.utils.data.Subset(dataset1, few_shot_indices)\n",
    "\n",
    "sup_train_loader = torch.utils.data.DataLoader(\n",
    "    sup_imgs_train, **nn_train_kwargs)\n",
    "\n",
    "optimizer = optim.Adadelta(model.parameters(), lr=1.0)\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=0.7)\n",
    "\n",
    "for epoch in range(1, 5):\n",
    "    train(model, device, sup_train_loader,\n",
    "        optimizer, epoch)\n",
    "    #test(model, device, nn_test_loader)\n",
    "    scheduler.step()\n",
    "test(model, device, nn_test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Abducing...: 100%|██████████| 3000/3000 [00:30<00:00, 97.97it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6000 (0%)]\tLoss: 1.958068\n",
      "Abduction accuracy:  0.6456666666666667\n",
      "Train Epoch: 2 [0/6000 (0%)]\tLoss: 1.296085\n",
      "Abduction accuracy:  0.6456666666666667\n",
      "-- Test set: Average loss: 0.0137, Accuracy: 7669/10000 (77%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Abducing...: 100%|██████████| 3000/3000 [00:29<00:00, 101.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6000 (0%)]\tLoss: 0.926988\n",
      "Abduction accuracy:  0.9103333333333333\n",
      "Train Epoch: 2 [0/6000 (0%)]\tLoss: 0.585736\n",
      "Abduction accuracy:  0.9103333333333333\n",
      "-- Test set: Average loss: 0.0046, Accuracy: 9210/10000 (92%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Abducing...: 100%|██████████| 3000/3000 [00:30<00:00, 99.83it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6000 (0%)]\tLoss: 0.608485\n",
      "Abduction accuracy:  0.9873333333333333\n",
      "Train Epoch: 2 [0/6000 (0%)]\tLoss: 0.344015\n",
      "Abduction accuracy:  0.9873333333333333\n",
      "-- Test set: Average loss: 0.0027, Accuracy: 9508/10000 (95%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Abducing...: 100%|██████████| 3000/3000 [00:29<00:00, 101.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6000 (0%)]\tLoss: 0.331251\n",
      "Abduction accuracy:  0.995\n",
      "Train Epoch: 2 [0/6000 (0%)]\tLoss: 0.349095\n",
      "Abduction accuracy:  0.995\n",
      "-- Test set: Average loss: 0.0024, Accuracy: 9554/10000 (96%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Abducing...: 100%|██████████| 3000/3000 [00:29<00:00, 100.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6000 (0%)]\tLoss: 0.333204\n",
      "Abduction accuracy:  0.9953333333333333\n",
      "Train Epoch: 2 [0/6000 (0%)]\tLoss: 0.350591\n",
      "Abduction accuracy:  0.9953333333333333\n",
      "-- Test set: Average loss: 0.0023, Accuracy: 9561/10000 (96%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Abducing...: 100%|██████████| 3000/3000 [00:29<00:00, 101.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6000 (0%)]\tLoss: 0.193948\n",
      "Abduction accuracy:  0.9956666666666667\n",
      "Train Epoch: 2 [0/6000 (0%)]\tLoss: 0.240049\n",
      "Abduction accuracy:  0.9956666666666667\n",
      "-- Test set: Average loss: 0.0023, Accuracy: 9576/10000 (96%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Abducing...: 100%|██████████| 3000/3000 [00:29<00:00, 101.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6000 (0%)]\tLoss: 0.218380\n",
      "Abduction accuracy:  0.9956666666666667\n",
      "Train Epoch: 2 [0/6000 (0%)]\tLoss: 0.282037\n",
      "Abduction accuracy:  0.9956666666666667\n",
      "-- Test set: Average loss: 0.0022, Accuracy: 9574/10000 (96%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Abducing...: 100%|██████████| 3000/3000 [00:29<00:00, 101.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6000 (0%)]\tLoss: 0.307142\n",
      "Abduction accuracy:  0.9956666666666667\n",
      "Train Epoch: 2 [0/6000 (0%)]\tLoss: 0.427967\n",
      "Abduction accuracy:  0.9956666666666667\n",
      "-- Test set: Average loss: 0.0022, Accuracy: 9578/10000 (96%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Abducing...: 100%|██████████| 3000/3000 [00:29<00:00, 101.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6000 (0%)]\tLoss: 0.305546\n",
      "Abduction accuracy:  0.9956666666666667\n",
      "Train Epoch: 2 [0/6000 (0%)]\tLoss: 0.310958\n",
      "Abduction accuracy:  0.9956666666666667\n",
      "-- Test set: Average loss: 0.0022, Accuracy: 9580/10000 (96%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Abducing...: 100%|██████████| 3000/3000 [00:29<00:00, 102.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6000 (0%)]\tLoss: 0.294766\n",
      "Abduction accuracy:  0.9956666666666667\n",
      "Train Epoch: 2 [0/6000 (0%)]\tLoss: 0.306095\n",
      "Abduction accuracy:  0.9956666666666667\n",
      "-- Test set: Average loss: 0.0022, Accuracy: 9579/10000 (96%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Abducing...: 100%|██████████| 3000/3000 [00:29<00:00, 100.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6000 (0%)]\tLoss: 0.367064\n",
      "Abduction accuracy:  0.9956666666666667\n",
      "Train Epoch: 2 [0/6000 (0%)]\tLoss: 0.403212\n",
      "Abduction accuracy:  0.9956666666666667\n",
      "-- Test set: Average loss: 0.0022, Accuracy: 9579/10000 (96%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Abducing...: 100%|██████████| 3000/3000 [00:29<00:00, 100.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6000 (0%)]\tLoss: 0.338030\n",
      "Abduction accuracy:  0.9956666666666667\n",
      "Train Epoch: 2 [0/6000 (0%)]\tLoss: 0.229107\n",
      "Abduction accuracy:  0.9956666666666667\n",
      "-- Test set: Average loss: 0.0022, Accuracy: 9579/10000 (96%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Abducing...: 100%|██████████| 3000/3000 [00:30<00:00, 99.94it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6000 (0%)]\tLoss: 0.222177\n",
      "Abduction accuracy:  0.9956666666666667\n",
      "Train Epoch: 2 [0/6000 (0%)]\tLoss: 0.232248\n",
      "Abduction accuracy:  0.9956666666666667\n",
      "-- Test set: Average loss: 0.0022, Accuracy: 9579/10000 (96%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Abducing...: 100%|██████████| 3000/3000 [00:29<00:00, 100.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6000 (0%)]\tLoss: 0.355552\n",
      "Abduction accuracy:  0.9956666666666667\n",
      "Train Epoch: 2 [0/6000 (0%)]\tLoss: 0.440173\n",
      "Abduction accuracy:  0.9956666666666667\n",
      "-- Test set: Average loss: 0.0022, Accuracy: 9579/10000 (96%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Abducing...: 100%|██████████| 3000/3000 [00:30<00:00, 99.89it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6000 (0%)]\tLoss: 0.301352\n",
      "Abduction accuracy:  0.9956666666666667\n",
      "Train Epoch: 2 [0/6000 (0%)]\tLoss: 0.249726\n",
      "Abduction accuracy:  0.9956666666666667\n",
      "-- Test set: Average loss: 0.0022, Accuracy: 9579/10000 (96%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Abducing...: 100%|██████████| 3000/3000 [00:20<00:00, 145.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6000 (0%)]\tLoss: 0.252311\n",
      "Abduction accuracy:  0.9956666666666667\n",
      "Train Epoch: 2 [0/6000 (0%)]\tLoss: 0.270329\n",
      "Abduction accuracy:  0.9956666666666667\n",
      "-- Test set: Average loss: 0.0023, Accuracy: 9579/10000 (96%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Abducing...: 100%|██████████| 3000/3000 [00:26<00:00, 114.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6000 (0%)]\tLoss: 0.370963\n",
      "Abduction accuracy:  0.9956666666666667\n",
      "Train Epoch: 2 [0/6000 (0%)]\tLoss: 0.205726\n",
      "Abduction accuracy:  0.9956666666666667\n",
      "-- Test set: Average loss: 0.0022, Accuracy: 9579/10000 (96%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Abducing...: 100%|██████████| 3000/3000 [00:28<00:00, 104.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6000 (0%)]\tLoss: 0.236914\n",
      "Abduction accuracy:  0.9956666666666667\n",
      "Train Epoch: 2 [0/6000 (0%)]\tLoss: 0.317033\n",
      "Abduction accuracy:  0.9956666666666667\n",
      "-- Test set: Average loss: 0.0022, Accuracy: 9579/10000 (96%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Abducing...: 100%|██████████| 3000/3000 [00:19<00:00, 152.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6000 (0%)]\tLoss: 0.265046\n",
      "Abduction accuracy:  0.9956666666666667\n",
      "Train Epoch: 2 [0/6000 (0%)]\tLoss: 0.282606\n",
      "Abduction accuracy:  0.9956666666666667\n",
      "-- Test set: Average loss: 0.0022, Accuracy: 9579/10000 (96%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Abducing...: 100%|██████████| 3000/3000 [00:17<00:00, 170.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6000 (0%)]\tLoss: 0.353039\n",
      "Abduction accuracy:  0.9956666666666667\n",
      "Train Epoch: 2 [0/6000 (0%)]\tLoss: 0.300775\n",
      "Abduction accuracy:  0.9956666666666667\n",
      "-- Test set: Average loss: 0.0022, Accuracy: 9579/10000 (96%)\n"
     ]
    }
   ],
   "source": [
    "ABL_epochs = 20\n",
    "for epoch in range(ABL_epochs):\n",
    "    ABL_main(model, prolog, mnist_sum_data_train, optimizer=optimizer, scheduler=scheduler) #check"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0818a47565ceb6402158bbf0dc496aa25facf52e7106c9c4cd712582e3748063"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
